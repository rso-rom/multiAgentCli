# Backend Setup Guide

> **WÃ¤hle dein KI-Backend: LM Studio oder Ollama Docker**

## ğŸ¯ Welches Backend ist richtig fÃ¼r mich?

### Nutze **LM Studio** wenn du:
- âœ… Eine **grafische OberflÃ¤che** bevorzugst
- âœ… Auf **Windows** arbeitest
- âœ… **Desktop-Entwicklung** machst
- âœ… Modelle **visuell verwalten** mÃ¶chtest
- âœ… **Einfache Installation** willst

ğŸ‘‰ **[Zur LM Studio Anleitung](lm-studio.md)**

### Nutze **Ollama Docker** wenn du:
- âœ… **Docker** bereits nutzt
- âœ… **Server/Headless** Setup brauchst
- âœ… **CI/CD Integration** planst
- âœ… **Automatisierung** bevorzugst
- âœ… **CLI-basiert** arbeiten willst

ğŸ‘‰ **[Zur Ollama Docker Anleitung](ollama-docker.md)**

---

## ğŸ“Š Detaillierter Vergleich

| Kriterium | LM Studio | Ollama Docker | Gewinner |
|-----------|-----------|---------------|----------|
| **Einfachheit** | â­â­â­â­â­ | â­â­â­ | ğŸ† LM Studio |
| **GUI** | âœ… Ja | âŒ Nein | ğŸ† LM Studio |
| **Windows-Performance** | â­â­â­â­â­ | â­â­â­â­ | ğŸ† LM Studio |
| **Modell-Management** | â­â­â­â­â­ Visuell | â­â­â­â­ CLI | ğŸ† LM Studio |
| **Automatisierung** | â­â­ | â­â­â­â­â­ | ğŸ† Ollama |
| **Server-Nutzung** | â­â­ | â­â­â­â­â­ | ğŸ† Ollama |
| **CI/CD Integration** | âŒ | âœ… | ğŸ† Ollama |
| **Resource Overhead** | Niedrig | Mittel (Docker) | ğŸ† LM Studio |
| **Updates** | Automatisch (GUI) | `docker pull` | âš–ï¸ Unentschieden |

---

## ğŸš€ Schnellstart-Links

### LM Studio
```bash
# 1. Download & Install: https://lmstudio.ai/
# 2. Modell laden (GUI)
# 3. Server starten (GUI Button)
# 4. App konfigurieren:
OLLAMA_URL=http://localhost:1234/v1

# Fertig!
```
ğŸ“– **[VollstÃ¤ndige LM Studio Anleitung](lm-studio.md)**

### Ollama Docker
```bash
# 1. Docker Compose starten
docker-compose up -d ollama

# 2. Modell laden
docker exec -it codechat-ollama ollama pull mistral:7b

# 3. App konfigurieren:
OLLAMA_URL=http://localhost:11434

# Fertig!
```
ğŸ“– **[VollstÃ¤ndige Ollama Docker Anleitung](ollama-docker.md)**

---

## ğŸ’¡ Empfehlungen

### FÃ¼r AnfÃ¤nger
ğŸ‘‰ **LM Studio**
- Einfachste Installation
- Visuelle Modell-Verwaltung
- Kein Docker nÃ¶tig

### FÃ¼r Entwickler
ğŸ‘‰ **LM Studio** (Desktop) oder **Ollama Docker** (Server)
- LM Studio: Wenn du lokal entwickelst
- Ollama: Wenn du Remote/Server nutzt

### FÃ¼r DevOps/CI/CD
ğŸ‘‰ **Ollama Docker**
- Komplett automatisierbar
- Container-basiert
- Einfach in Pipelines integrierbar

---

## ğŸ“š VollstÃ¤ndige Dokumentation

| Anleitung | Inhalt | Link |
|-----------|--------|------|
| **LM Studio Setup** | Installation, Konfiguration, Modelle, Troubleshooting | [â†’ Anleitung](lm-studio.md) |
| **Ollama Docker Setup** | Docker Compose, Modelle, CLI, Automatisierung | [â†’ Anleitung](ollama-docker.md) |
| **Memory Testing** | Qdrant, 4-Ebenen-Memory, Semantic Search | [â†’ Anleitung](../features/memory-system.md) |
| **Quick Start** | 3-Minuten Einstieg | [â†’ Anleitung](../../QUICK-START.md) |

---

## ğŸ”„ Kann ich spÃ¤ter wechseln?

**Ja!** Beide Backends nutzen die gleiche API. Du kannst jederzeit wechseln:

### Von LM Studio zu Ollama Docker:
```env
# In .env Ã¤ndern:
OLLAMA_URL=http://localhost:11434  # statt 1234/v1
```

### Von Ollama Docker zu LM Studio:
```env
# In .env Ã¤ndern:
OLLAMA_URL=http://localhost:1234/v1  # /v1 hinzufÃ¼gen!
```

Kein Code-Ã„nderung nÃ¶tig! Nur .env anpassen und `npm run build`.

---

## âš™ï¸ Beide gleichzeitig nutzen?

**Ja!** Du kannst beide parallel laufen lassen:

**LM Studio**: Port 1234
**Ollama Docker**: Port 11434

Wechsle per Workflow:
```yaml
agents:
  fast-agent:
    backend: ollama
    model: llama3.2:3b  # Ollama Docker

  quality-agent:
    backend: ollama
    model: gpt-4  # LM Studio (via OpenAI API)
```

Oder in .env fÃ¼r verschiedene Projekte.

---

## ğŸ†˜ Hilfe & Support

- **LM Studio Probleme**: [LM Studio Discord](https://discord.gg/lmstudio)
- **Ollama Probleme**: [Ollama GitHub](https://github.com/ollama/ollama/issues)
- **App Probleme**: Siehe [README.md](../README.md)

---

## âœ… Zusammenfassung

| WÃ¤hle | Wenn du |
|-------|---------|
| **LM Studio** | GUI magst, Windows nutzt, Desktop-Entwicklung |
| **Ollama Docker** | CLI bevorzugst, Server/Automation brauchst |
| **Beide** | Verschiedene Setups fÃ¼r verschiedene Projekte |

**Los geht's!** WÃ¤hle eine Anleitung oben und starte in 5 Minuten! ğŸš€
