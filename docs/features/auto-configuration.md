# ğŸ¤– Auto-Configuration - Self-Configuring Backend System

## ğŸ¯ Ãœbersicht

**Das System konfiguriert sich selbst!** cacli kann ein bereits konfiguriertes LLM nutzen, um automatisch neue Backends zu integrieren - ohne dass du Code schreiben musst.

### Wie funktioniert es?

1. **Research**: Ein bereits konfiguriertes Model (z.B. Ollama) recherchiert die API-Struktur des neuen Backends
2. **Generate**: Das System generiert automatisch den TypeScript-Code
3. **Configure**: Environment-Variablen werden automatisch gesetzt
4. **Test**: Die Verbindung wird getestet

**Das ist Meta-Programming:** Code, der Code schreibt! ğŸš€

---

## ğŸ’¡ Warum Auto-Configuration?

### Problem: Backend-Integration ist aufwÃ¤ndig

Normalerweise musst du:
1. âŒ API-Dokumentation lesen
2. âŒ Backend-Klasse schreiben
3. âŒ Config-Dateien anpassen
4. âŒ Environment-Variablen setzen
5. âŒ Testen und Debuggen

**Zeitaufwand:** ~2-4 Stunden pro Backend

### LÃ¶sung: Auto-Configuration

Mit Auto-Configuration:
1. âœ… `cacli configure backend gemini`
2. âœ… API-Key eingeben
3. âœ… **Fertig!**

**Zeitaufwand:** ~2 Minuten! ğŸ‰

---

## ğŸš€ Verwendung

### Methode 1: Direkter Befehl

```bash
# Backend mit API-Key konfigurieren
cacli configure backend gemini --api-key YOUR_API_KEY

# Backend ohne API-Key (wird spÃ¤ter gesetzt)
cacli configure backend mistral
```

### Methode 2: Interaktiver Wizard

```bash
cacli configure interactive

# oder
cacli configure wizard
```

**Interaktiver Ablauf:**

```
ğŸ¤– Auto-Configuration Wizard

? Which backend would you like to configure?
  â¯ Gemini
    Mistral
    Cohere
    Huggingface
    Replicate
    Together
    Perplexity
    Groq
    Custom (enter manually)

? Enter your gemini API key (optional): â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢

ğŸ” Researching gemini API...
âœ… Research complete!
   API URL: https://generativelanguage.googleapis.com/v1beta
   Auth: api-key
   Default Model: gemini-pro
   Streaming: Yes
   Vision: Yes

? Generate backend implementation? Yes

ğŸ”¨ Generating backend code...
âœ… Saved: src/backends/gemini.ts

ğŸ”§ Updating configuration files...
âœ… Configuration files updated

âš™ï¸  Configuring environment...
âœ… Updated .env.example
âœ… Updated .env with API key

ğŸ§ª Testing connection...
âœ… Connection successful!
   Response: Hello from Gemini! I'm ready to help...

ğŸ‰ Auto-configuration complete!

ğŸ“ Next steps:
   1. Review generated code: src/backends/gemini.ts
   2. Set API key: GEMINI_API_KEY in .env
   3. Test: cacli -b gemini
```

### Methode 3: Liste verfÃ¼gbarer Backends

```bash
cacli configure list

# Output:
ğŸ¤– Backends that can be auto-configured:

  â€¢ Gemini
  â€¢ Mistral
  â€¢ Cohere
  â€¢ Huggingface
  â€¢ Replicate
  â€¢ Together
  â€¢ Perplexity
  â€¢ Groq

Usage: cacli configure backend <name>
   or: cacli configure interactive
```

---

## ğŸ” Wie funktioniert es technisch?

### Schritt 1: Research Phase

Das System nutzt ein bereits konfiguriertes LLM (z.B. Ollama mit llama3):

```typescript
const prompt = `Research the ${backendName} API and provide:

API_URL: [base endpoint]
AUTH_TYPE: [api-key, oauth, or none]
DEFAULT_MODEL: [recommended model]
SUPPORTS_VISION: [YES or NO]
SUPPORTS_STREAMING: [YES or NO]`;

const response = await llm.chat(prompt);
```

**Beispiel-Response:**

```
API_URL: https://api.mistral.ai/v1
AUTH_TYPE: api-key
DEFAULT_MODEL: mistral-medium
SUPPORTS_VISION: NO
SUPPORTS_STREAMING: YES
```

### Schritt 2: Code Generation

Das LLM generiert die komplette Backend-Implementierung:

```typescript
const prompt = `Generate TypeScript code for a ${backendName} backend.

Requirements:
- Class: ${className}
- Extends: ModelBackend
- API URL: ${config.apiUrl}
- Auth: ${config.authType}
- Streaming: ${config.supportsStreaming}
- Vision: ${config.supportsVision}

Generate ONLY TypeScript code, no explanations.`;

const code = await llm.chat(prompt);
```

**Generierter Code:**

```typescript
import axios from 'axios';
import { ModelBackend, StreamCallback } from './base';

export class GeminiBackend extends ModelBackend {
  private apiKey: string;
  private model: string;
  private apiUrl = 'https://generativelanguage.googleapis.com/v1beta';

  constructor(apiKey?: string, model = 'gemini-pro') {
    super();
    if (!apiKey) {
      throw new Error('API key required');
    }
    this.apiKey = apiKey;
    this.model = model;
  }

  async chat(prompt: string, onStream?: StreamCallback): Promise<string | void> {
    // Implementation...
  }
}
```

### Schritt 3: Integration

Das System aktualisiert automatisch:

**src/config.ts:**
```typescript
// Auto-generated import
import { GeminiBackend } from './backends/gemini';

export type BackendName = '...' | 'gemini';

export function getBackend(name?: string) {
  // ...
  if (backend === 'gemini') {
    return new GeminiBackend(
      process.env.GEMINI_API_KEY,
      process.env.GEMINI_MODEL || 'gemini-pro'
    );
  }
}
```

**src/orchestrator/backend-selector.ts:**
```typescript
// Auto-generated detection
if (process.env.GEMINI_API_KEY) {
  options.push({
    name: 'Gemini (gemini-pro)',
    backend: 'gemini',
    model: 'gemini-pro',
    available: true,
    description: 'Google\'s multimodal AI',
    cost: 'Paid'
  });
}
```

**.env.example:**
```env
# Gemini Configuration
GEMINI_API_KEY=
GEMINI_MODEL=gemini-pro
```

### Schritt 4: Testing

Das System testet die Verbindung:

```typescript
const backend = new GeminiBackend(apiKey, model);
const response = await backend.chat('Say "Hello from gemini!"');

if (response) {
  console.log('âœ… Connection successful!');
} else {
  console.log('âš ï¸  Test failed');
}
```

---

## ğŸ“Š UnterstÃ¼tzte Backends

### Vorkonfiguriert

Diese Backends sind bereits vorbereitet:

| Backend | Provider | Hauptfeature | Kosten |
|---------|----------|--------------|--------|
| **gemini** | Google | Multimodal, Vision | Paid |
| **mistral** | Mistral AI | Schnell, FranzÃ¶sisch | Paid |
| **cohere** | Cohere | Embeddings, RAG | Paid |
| **huggingface** | Hugging Face | Open Source Models | Free/Paid |
| **replicate** | Replicate | Community Models | Paid |
| **together** | Together AI | Open Source | Paid |
| **perplexity** | Perplexity | Web-Search-fÃ¤hig | Paid |
| **groq** | Groq | Ultra-schnell | Paid |

### Custom Backends

Du kannst auch **beliebige andere Backends** hinzufÃ¼gen:

```bash
cacli configure interactive

? Which backend would you like to configure?
  â¯ Custom (enter manually)

? Enter backend name: fireworks

ğŸ” Researching fireworks API...
# System recherchiert automatisch die API
```

---

## ğŸ¯ Use Cases

### Use Case 1: Schnelles Experimentieren

**Situation:** Du willst ein neues Model ausprobieren

**Ohne Auto-Configuration:**
```bash
# 2 Stunden Arbeit:
# 1. Dokumentation lesen
# 2. Backend-Klasse schreiben
# 3. Tests schreiben
# 4. Debuggen
```

**Mit Auto-Configuration:**
```bash
cacli configure backend gemini --api-key YOUR_KEY
# 2 Minuten! âœ…
cacli -b gemini
```

### Use Case 2: Multi-Backend Setup

**Situation:** Du willst mehrere Backends fÃ¼r verschiedene Tasks

```bash
# Alle Backends in 10 Minuten konfigurieren:
cacli configure backend gemini --api-key KEY1
cacli configure backend mistral --api-key KEY2
cacli configure backend cohere --api-key KEY3

# Jetzt verfÃ¼gbar:
cacli -b gemini    # Vision-Tasks
cacli -b mistral   # Schnelle Tasks
cacli -b cohere    # RAG/Embeddings
```

### Use Case 3: Team-Onboarding

**Situation:** Neues Teammitglied will cacli nutzen

**Ohne Auto-Configuration:**
```
ğŸ“– Lies die 20-seitige Dokumentation
âŒ¨ï¸  Schreibe Backend-Code
ğŸ› Debug Fehler
â° Zeit: 4-6 Stunden
```

**Mit Auto-Configuration:**
```bash
cacli configure interactive
# Wizard fÃ¼hrt durch die Konfiguration
# Zeit: 5 Minuten! âœ…
```

### Use Case 4: Neue API-Version

**Situation:** Backend-Provider released neue API-Version

```bash
# Alte Version entfernen:
rm src/backends/gemini-old.ts

# Neue Version auto-generieren:
cacli configure backend gemini --api-key YOUR_KEY

# System generiert Code fÃ¼r die neueste API! âœ…
```

---

## ğŸ› ï¸ Anforderungen

### Mindestanforderungen

1. **Ein konfiguriertes Backend**
   - Ollama (empfohlen, kostenlos)
   - OpenAI
   - Claude
   - OpenWebUI
   - Jedes andere

2. **Internet-Verbindung**
   - FÃ¼r API-Research
   - FÃ¼r Backend-Tests

### Empfohlene Konfiguration

```env
# Mindestens ein Backend:
MODEL_BACKEND=ollama
OLLAMA_MODEL=llama3

# Optional: Bessere Models fÃ¼r Code-Generation:
MODEL_BACKEND=claude
ANTHROPIC_USE_OAUTH=true
```

**Tipp:** FÃ¼r beste Code-Generierung nutze Claude oder GPT-4!

---

## ğŸ¨ Erweiterte Nutzung

### Custom Model fÃ¼r Generation

Nutze ein spezifisches Model fÃ¼r die Code-Generierung:

```typescript
import { AutoConfigurator } from './setup/auto-configurator';
import { getBackend } from './config';

// Nutze Claude fÃ¼r bessere Code-QualitÃ¤t
const llm = getBackend('claude');
const configurator = new AutoConfigurator(llm);

await configurator.configure('gemini', 'YOUR_API_KEY');
```

### Batch-Configuration

Konfiguriere mehrere Backends auf einmal:

```typescript
const backends = ['gemini', 'mistral', 'cohere'];
const apiKeys = {
  gemini: 'KEY1',
  mistral: 'KEY2',
  cohere: 'KEY3'
};

for (const backend of backends) {
  await configurator.configure(backend, apiKeys[backend]);
}
```

### Dry-Run Mode

Generiere Code ohne zu speichern:

```typescript
const config = await configurator.researchBackend('gemini');
const code = await configurator.generateBackendCode('gemini', config);

console.log('Generated code:');
console.log(code);

// Code wird nicht gespeichert
```

---

## ğŸ“ Generierte Dateien

Nach Auto-Configuration werden folgende Dateien **automatisch** erstellt/aktualisiert:

### Neue Dateien

```
src/backends/gemini.ts          # Backend-Implementierung
```

### Aktualisierte Dateien

```
src/config.ts                   # Backend-Registration
src/orchestrator/backend-selector.ts  # Backend-Detection
.env.example                    # Environment-Template
.env                           # Deine Konfiguration
```

---

## ğŸ§ª Testing

### Manuelle Tests

Nach der Konfiguration:

```bash
# 1. Backend verfÃ¼gbar?
cacli configure list

# 2. Connection testen:
cacli -b gemini ask "Hello!"

# 3. Im REPL:
cacli -b gemini
> Hello from Gemini!
```

### Automatische Tests

Das System fÃ¼hrt automatisch Tests durch:

```typescript
âœ… Research successful
âœ… Code generation successful
âœ… Files saved
âœ… Configuration updated
âœ… Environment configured
ğŸ§ª Testing connection...
âœ… Connection successful!
```

Falls ein Test fehlschlÃ¤gt:

```
âš ï¸  Connection test failed: Invalid API key
   This is normal - you may need to adjust the generated code
```

---

## ğŸ”§ Troubleshooting

### Problem: Research schlÃ¤gt fehl

**Symptom:**
```
âŒ Could not research backend API
```

**LÃ¶sung:**
1. PrÃ¼fe Internet-Verbindung
2. Stelle sicher, dass ein Backend konfiguriert ist
3. Nutze ein besseres Model (Claude statt Ollama)

```bash
# Besseres Model nutzen:
MODEL_BACKEND=claude cacli configure backend gemini
```

### Problem: Code-Generierung fehlerhaft

**Symptom:**
```
âš ï¸  Test failed: Syntax error
```

**LÃ¶sung:**
1. Review den generierten Code in `src/backends/gemini.ts`
2. Manuell korrigieren
3. Oder erneut generieren mit besserem Model

```bash
# Code-QualitÃ¤t verbessern:
cacli configure backend gemini --api-key KEY
# Nutzt das beste verfÃ¼gbare Model
```

### Problem: API-Key falsch

**Symptom:**
```
âš ï¸  Test failed: 401 Unauthorized
```

**LÃ¶sung:**
1. PrÃ¼fe API-Key in `.env`
2. Generiere neuen Key beim Provider
3. Update .env und test erneut

```bash
# .env
GEMINI_API_KEY=your-new-api-key-here

# Test:
cacli -b gemini ask "Test"
```

### Problem: Backend existiert bereits

**Symptom:**
```
âŒ Backend file already exists
```

**LÃ¶sung:**
1. Backup der alten Datei erstellen
2. Datei lÃ¶schen oder umbenennen
3. Erneut konfigurieren

```bash
# Backup:
mv src/backends/gemini.ts src/backends/gemini.old.ts

# Neu generieren:
cacli configure backend gemini
```

---

## ğŸ¯ Best Practices

### 1. Nutze gute Models fÃ¼r Generation

**Empfehlung:**
- âœ… Claude 3.5 Sonnet (beste Code-QualitÃ¤t)
- âœ… GPT-4 (sehr gut)
- âš ï¸  Llama3 (ok, manchmal Fehler)
- âŒ Kleine Models (<7B) - zu unzuverlÃ¤ssig

```env
# FÃ¼r Auto-Configuration:
MODEL_BACKEND=claude
ANTHROPIC_USE_OAUTH=true
```

### 2. Review generierten Code

Auch wenn das System Code generiert - **immer reviewen!**

```bash
# Nach Generierung:
cat src/backends/gemini.ts

# PrÃ¼fe:
# - Imports korrekt?
# - Error Handling vorhanden?
# - Streaming implementiert?
# - Types korrekt?
```

### 3. Version Control

Committe generierten Code:

```bash
git add src/backends/gemini.ts
git commit -m "feat: add Gemini backend (auto-generated)"

# Falls Ã„nderungen nÃ¶tig:
git add src/backends/gemini.ts
git commit -m "fix: adjust Gemini API calls"
```

### 4. Teste grÃ¼ndlich

```bash
# Unit-Tests:
npm test src/backends/gemini.test.ts

# Integration-Tests:
cacli -b gemini ask "Hello!"
cacli -b gemini  # REPL-Test

# Vision-Test (falls unterstÃ¼tzt):
cacli -b gemini vision "Describe this image" image.jpg
```

### 5. Dokumentiere Custom Backends

```markdown
# docs/backends/gemini.md

## Gemini Backend

Auto-generated: 2024-01-15
Model: gemini-pro

### Configuration
\`\`\`env
GEMINI_API_KEY=your-key
GEMINI_MODEL=gemini-pro
\`\`\`

### Features
- âœ… Chat
- âœ… Streaming
- âœ… Vision
- âŒ Function Calling
```

---

## ğŸ“Š Vergleich: Manuell vs. Auto-Configuration

| Aspekt | Manuell | Auto-Configuration |
|--------|---------|-------------------|
| **Zeit** | 2-4 Stunden | 2-5 Minuten |
| **Code-QualitÃ¤t** | Hoch (wenn erfahren) | Mittel-Hoch |
| **FehleranfÃ¤lligkeit** | Mittel | Niedrig |
| **Dokumentation** | Manuell schreiben | Auto-generiert |
| **API-Updates** | Manuell anpassen | Neu generieren |
| **Learning Curve** | Steil | Flach |
| **Customization** | Voll | Begrenzt |

**Empfehlung:**
- ğŸš€ **Prototyping:** Auto-Configuration
- ğŸ—ï¸ **Produktion:** Auto-Configuration + Review
- ğŸ¯ **Custom Features:** Manuell erweitern

---

## ğŸ”® ZukÃ¼nftige Features

### In Planung

- [ ] **Model-Switcher fÃ¼r Generation**
  ```bash
  cacli configure backend gemini --generator-model claude
  ```

- [ ] **Dry-Run Mode**
  ```bash
  cacli configure backend gemini --dry-run
  # Zeigt generierten Code, speichert nicht
  ```

- [ ] **Template System**
  ```bash
  cacli configure backend gemini --template openai-compatible
  # Nutzt Template fÃ¼r Ã¤hnliche APIs
  ```

- [ ] **Update Command**
  ```bash
  cacli configure update gemini
  # Regeneriert fÃ¼r neue API-Version
  ```

- [ ] **Batch Configuration**
  ```bash
  cacli configure batch gemini,mistral,cohere
  # Konfiguriert mehrere auf einmal
  ```

---

## ğŸ‰ Zusammenfassung

### Was ist Auto-Configuration?

Ein **selbst-konfigurierendes System**, das:
- âœ… Ein bestehendes LLM nutzt, um neue Backends zu recherchieren
- âœ… Automatisch TypeScript-Code generiert
- âœ… Konfigurationsdateien aktualisiert
- âœ… Environment-Variablen setzt
- âœ… Verbindungen testet

### Warum Auto-Configuration?

- â±ï¸ **Zeit:** 2 Minuten statt 4 Stunden
- ğŸ¯ **Einfachheit:** Ein Befehl statt 10 Dateien
- ğŸš€ **Geschwindigkeit:** Sofort loslegen
- ğŸ”„ **Updates:** Neu generieren statt manuell anpassen

### Wie nutze ich es?

```bash
# Einfach:
cacli configure backend gemini --api-key YOUR_KEY

# Interaktiv:
cacli configure interactive

# Liste:
cacli configure list
```

### Wann nutze ich es?

- âœ… Neues Backend ausprobieren
- âœ… Team-Onboarding
- âœ… Prototyping
- âœ… API-Updates
- âš ï¸  Production (mit Review!)

---

## ğŸ“š NÃ¤chste Schritte

1. **Ausprobieren:**
   ```bash
   cacli configure interactive
   ```

2. **Dokumentation:**
   - [Backend hinzufÃ¼gen (manuell)](adding-new-backend.md)
   - [Multi-Backend Setup](multi-backend-agents.md)
   - [Setup Wizard](../setup/setup-wizard.md)

3. **Support:**
   - GitHub Issues: [Report a Bug](https://github.com/your-repo/issues)
   - Discussions: [Ask Questions](https://github.com/your-repo/discussions)

ğŸš€ **Viel Erfolg mit Auto-Configuration!**
