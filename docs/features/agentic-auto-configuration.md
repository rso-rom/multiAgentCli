# ğŸ¤– Agentic Auto-Configuration - LLM mit Tool-Nutzung

## ğŸ¯ Ãœbersicht

**Das LLM ist jetzt ein Agent!** Statt dass wir im Code Web-Requests machen, kann das LLM **selbst Tools wie curl/wget nutzen**, um APIs zu erforschen.

### Was ist Agentic AI?

**Agentic AI** = AI, die **Tools nutzen** kann, um Aufgaben selbststÃ¤ndig zu lÃ¶sen.

**Vorher (Passive AI):**
```
Wir: "Recherchiere Gemini API"
LLM: "Basierend auf meinem Training..."
     [Nutzt veraltetes Wissen]
```

**Jetzt (Agentic AI):**
```
Wir: "Recherchiere Gemini API"
LLM: "Ich werde curl nutzen:"
     [TOOL:curl:https://docs.gemini.ai/api-reference]
     [System fÃ¼hrt curl aus]
LLM: "Basierend auf der aktuellen Dokumentation..."
     [Nutzt Live-Daten!]
```

---

## ğŸš€ Wie funktioniert es?

### 1. LLM bekommt Tools

Das LLM wird informiert, welche Tools verfÃ¼gbar sind:

```typescript
Tools available:
- curl: Fetch web content
- wget: Download files
- http_get: Simple HTTP requests

Format: [TOOL:curl:https://example.com]
```

### 2. LLM entscheidet selbst

```
LLM: "To research the Gemini API, I will first fetch the docs:
      [TOOL:curl:https://docs.gemini.ai/api-reference]

      Then check the API endpoint:
      [TOOL:http_get:https://generativelanguage.googleapis.com/v1beta/models]"
```

### 3. System fÃ¼hrt Tools aus

```typescript
ğŸ”§ Executing tool: curl https://docs.gemini.ai/api-reference
âœ… Tool executed successfully (15432 bytes)

ğŸ”§ Executing tool: http_get https://generativelanguage.googleapis.com/v1beta/models
âœ… Tool executed successfully (2891 bytes)
```

### 4. LLM analysiert Ergebnisse

```
LLM: "Based on the documentation and API response:

      API_URL: https://generativelanguage.googleapis.com/v1beta
      AUTH_TYPE: api-key
      DEFAULT_MODEL: gemini-pro
      SUPPORTS_VISION: YES
      SUPPORTS_STREAMING: YES"
```

---

## ğŸ’¡ 3 Modi verfÃ¼gbar

### Modus 1: **Agentic Tool Use** (Standard, Beste QualitÃ¤t)

```bash
cacli configure backend gemini --api-key KEY
```

**Was passiert:**
```
ğŸ¤– Agentic Tool Use: Enabled (LLM can use curl/wget)
ğŸ”„ Agentic iteration 1/3...
ğŸ”§ Executing 2 tool call(s)...
  âœ… curl:https://docs.gemini.ai/api-reference
  âœ… http_get:https://generativelanguage.googleapis.com/v1beta/models
ğŸ”„ Agentic iteration 2/3...
âœ… Research complete!
```

**Vorteile:**
- âœ… LLM entscheidet selbst, welche URLs wichtig sind
- âœ… Kann mehrere Iterationen machen
- âœ… Passt sich dynamisch an
- âœ… Beste Ergebnisse

### Modus 2: **Web Search** (Pre-Fetch)

```bash
cacli configure backend gemini --api-key KEY --no-agentic-tools
```

**Was passiert:**
```
ğŸŒ Web Search: Enabled (pre-fetch documentation)
ğŸŒ Searching web for gemini API documentation...
âœ… Found documentation at: https://docs.gemini.ai/api-reference
ğŸ” Searching GitHub for gemini examples...
âœ… Found 847 GitHub examples
```

**Vorteile:**
- âœ… Schneller (keine Iterationen)
- âœ… Weniger LLM-Calls
- âš ï¸  Weniger flexibel

### Modus 3: **LLM Knowledge Only** (Offline)

```bash
cacli configure backend gemini --api-key KEY --no-web-search
```

**Was passiert:**
```
ğŸ“š LLM Knowledge Only: No web access
[Nutzt nur Training-Daten]
```

**Vorteile:**
- âœ… Funktioniert offline
- âœ… Am schnellsten
- âš ï¸  Kann veraltet sein

---

## ğŸ“Š Modi-Vergleich

| Feature | Agentic ğŸ¤– | Web Search ğŸŒ | LLM Only ğŸ“š |
|---------|-----------|---------------|-------------|
| **Genauigkeit** | â­â­â­â­â­ | â­â­â­â­ | â­â­â­ |
| **AktualitÃ¤t** | Live-Daten | Live-Daten | Training-Cutoff |
| **FlexibilitÃ¤t** | LLM entscheidet | Vordefiniert | Keine |
| **Iterationen** | Ja (max 3) | Nein | Nein |
| **Geschwindigkeit** | â­â­ (~10-20s) | â­â­â­ (~5-10s) | â­â­â­â­â­ (~2s) |
| **LLM-Calls** | 1-3 | 1 | 1 |
| **Internet** | Erforderlich | Erforderlich | Nicht nÃ¶tig |

**Empfehlung:** Agentic Tool Use fÃ¼r beste Ergebnisse!

---

## ğŸ” Technische Details

### Tool-Format

Das LLM nutzt folgendes Format fÃ¼r Tool-Calls:

```
[TOOL:tool_name:arguments]
```

**Beispiele:**

```
[TOOL:curl:https://docs.gemini.ai/api-reference]
[TOOL:wget:https://api.example.com/docs.html]
[TOOL:http_get:https://api.example.com/v1/models]
```

### Agentic Loop

```typescript
while (iteration < maxIterations) {
  // 1. LLM gibt Antwort (mit Tool-Calls)
  const response = await llm.chat(prompt);

  // 2. Parse Tool-Calls
  const toolCalls = parseToolCalls(response);

  // 3. FÃ¼hre Tools aus
  const results = await executeTools(toolCalls);

  // 4. Gib Feedback an LLM
  prompt = `Tool results: ${results}\n\nAnalyze and continue...`;

  // 5. Repeat oder finish
}
```

### Sicherheit

**Sanitization:**
```typescript
// Entfernt gefÃ¤hrliche Zeichen:
- ; | & ` $ ( )  // Shell-Operators
- --exec, --config  // GefÃ¤hrliche Flags

// Erlaubt nur sichere curl/wget Optionen
```

**Timeout:**
```typescript
timeout: 10000ms  // 10 Sekunden max
maxBuffer: 10000  // Max 10KB Output
```

**Output-Limiting:**
```typescript
// Output wird auf 10KB begrenzt
if (output.length > 10000) {
  output = output.substring(0, 10000) + '\n[truncated]';
}
```

---

## ğŸ¯ Use Cases

### Use Case 1: Unbekanntes Backend

**Szenario:** Du willst ein neues, unbekanntes Backend nutzen.

```bash
cacli configure backend fireworks --api-key KEY
```

**Agentic LLM:**
```
ğŸ¤– "I don't have training data for 'fireworks', let me research:

[TOOL:curl:https://docs.fireworks.ai/api-reference]
[TOOL:http_get:https://api.fireworks.ai/v1/models]

After analyzing the docs, I found:
API_URL: https://api.fireworks.ai/v1
..."
```

**Ergebnis:** Auch unbekannte Backends funktionieren! âœ…

### Use Case 2: API-Version-Update

**Szenario:** Backend hat API aktualisiert.

**Mit Agentic:**
```bash
cacli configure backend gemini  # Holt neueste Version!
```

Das LLM fetcht immer die **aktuelle** Dokumentation.

### Use Case 3: Custom Endpoints

**Szenario:** Backend nutzt non-standard URLs.

**Agentic LLM probiert mehrere:**
```
[TOOL:curl:https://docs.backend.ai/api-reference]  # âŒ 404
[TOOL:curl:https://backend.ai/docs/api]  # âŒ 404
[TOOL:curl:https://developers.backend.com/api]  # âœ… 200
```

Findet automatisch die richtige URL!

---

## ğŸ› ï¸ VerfÃ¼gbare Tools

### 1. curl

```bash
[TOOL:curl:https://api.example.com/docs]
```

**Verwendet fÃ¼r:**
- API-Dokumentation abrufen
- Endpoints testen
- HTML/JSON-Responses

**Optionen:**
```bash
[TOOL:curl:https://api.example.com/v1/models -H "Accept: application/json"]
```

### 2. wget

```bash
[TOOL:wget:https://example.com/documentation.html]
```

**Verwendet fÃ¼r:**
- Dokumentations-Downloads
- Static Files
- GroÃŸe Responses

### 3. http_get

```bash
[TOOL:http_get:https://api.example.com/v1/info]
```

**Verwendet fÃ¼r:**
- Einfache GET-Requests
- API-Tests
- Quick Checks

---

## ğŸ“ Beispiel-Ablauf

### Full Agentic Workflow

```bash
$ cacli configure backend mistral --api-key YOUR_KEY

ğŸ¯ Configuration mode:
   ğŸ¤– Agentic Tool Use: Enabled (LLM can use curl/wget)

ğŸ¤– Auto-configuring backend: mistral
ğŸ“¡ Using OllamaBackend to research and generate code...

ğŸ” Researching mistral API...
ğŸ¤– Using agentic tool-based research...
ğŸ¤– Starting agentic research for mistral...

ğŸ”„ Agentic iteration 1/3...
ğŸ”§ Executing 2 tool call(s)...
  ğŸ”§ Executing tool: curl https://docs.mistral.ai/api-reference
  âœ… Tool executed successfully (8523 bytes)
  ğŸ”§ Executing tool: http_get https://api.mistral.ai/v1/models
  âœ… Tool executed successfully (1842 bytes)

ğŸ”„ Agentic iteration 2/3...

âœ… Research complete!
   API URL: https://api.mistral.ai/v1
   Auth: api-key
   Default Model: mistral-medium
   Streaming: Yes
   Vision: No

? Generate backend implementation? Yes

ğŸ”¨ Generating backend code...
ğŸŒ Searching for mistral code examples...
âœ… Found official SDK: mistral-sdk

âœ… Saved: src/backends/mistral.ts

ğŸ”§ Updating configuration files...
âœ… Configuration files updated

âš™ï¸  Configuring environment...
âœ… Updated .env.example
âœ… Updated .env with API key

ğŸ§ª Testing connection...
âœ… Connection successful!

ğŸ‰ Auto-configuration complete!
```

---

## ğŸ”§ Erweiterte Nutzung

### Custom Tool Timeout

```typescript
const toolExecutor = new ToolExecutor();
toolExecutor.timeout = 20000;  // 20 Sekunden
```

### Mehr Iterationen erlauben

```typescript
// In auto-configurator.ts:
const maxIterations = 5;  // Standard: 3
```

### Debug-Modus

```bash
DEBUG=1 cacli configure backend gemini

# Zeigt alle Tool-Calls und Responses
```

---

## ğŸ¨ LLM-Verhalten

### Was das LLM typischerweise macht:

**Iteration 1:**
```
"I'll start by fetching the main documentation:"
[TOOL:curl:https://docs.gemini.ai/api-reference]
```

**Iteration 2:**
```
"Based on the docs, let me check available models:"
[TOOL:http_get:https://generativelanguage.googleapis.com/v1beta/models]
```

**Iteration 3:**
```
"Now I have all information needed:
API_URL: ...
AUTH_TYPE: ...
..."
```

### Fallback-Logik:

1. **Iteration 1 failed?** â†’ Versucht alternative URLs
2. **Iteration 2 failed?** â†’ Nutzt verfÃ¼gbare Infos
3. **Iteration 3 failed?** â†’ Fallback auf Web-Search-Modus

---

## âš™ï¸ Konfiguration

### Agentic deaktivieren

```bash
# Nutze Web-Search statt Agentic:
cacli configure backend gemini --no-agentic-tools

# Nutze nur LLM-Wissen:
cacli configure backend gemini --no-web-search
```

### Programmatisch

```typescript
import { AutoConfigurator } from './setup/auto-configurator';

// Agentic aktiviert:
const configurator = new AutoConfigurator(
  llm,          // LLM Backend
  true,         // Web Search
  true          // Agentic Tools
);

// Nur Web Search:
const configurator = new AutoConfigurator(llm, true, false);

// Nur LLM:
const configurator = new AutoConfigurator(llm, false, false);
```

---

## ğŸ§ª Testing

### Test 1: Agentic Mode

```bash
cacli configure backend gemini --api-key KEY

# Erwartung:
# - LLM macht Tool-Calls
# - curl/wget werden ausgefÃ¼hrt
# - 1-3 Iterationen
# - Erfolgreiche Konfiguration
```

### Test 2: Tool-Call Parsing

```typescript
const response = `
I'll fetch the docs:
[TOOL:curl:https://docs.example.com]

Then check the API:
[TOOL:http_get:https://api.example.com/v1]
`;

const calls = toolExecutor.parseToolCalls(response);
// Ergebnis: 2 Tool-Calls erkannt
```

### Test 3: Fallback

```bash
# Ollama offline â†’ Fallback auf LLM-Wissen
pkill ollama
cacli configure backend mistral

# Sollte trotzdem funktionieren (mit veralteten Daten)
```

---

## ğŸ“ Fazit

### Das System ist jetzt:

1. **ğŸ¤– Agentic** - LLM nutzt Tools selbststÃ¤ndig
2. **ğŸŒ Connected** - Holt Live-Daten aus dem Internet
3. **ğŸ”„ Iterative** - Kann mehrfach nachfragen
4. **ğŸ¯ Smart** - Passt sich an verschiedene APIs an
5. **ğŸ›¡ï¸ Sicher** - Sanitization & Timeouts

### Workflow:

```
Nutzer: "Configure Gemini"
    â†“
LLM: "Ich nutze curl..."
    â†“
System: [FÃ¼hrt curl aus]
    â†“
LLM: "Basierend auf der Doku..."
    â†“
System: [Generiert Code]
    â†“
âœ… Fertig!
```

### NÃ¤chste Schritte:

1. **Ausprobieren:**
   ```bash
   cacli configure backend gemini --api-key YOUR_KEY
   ```

2. **Vergleichen:**
   ```bash
   # Agentic vs. Web Search vs. LLM Only
   ```

3. **Erweitern:**
   - Weitere Tools hinzufÃ¼gen (git, npm, etc.)
   - Mehr Iterationen erlauben
   - Custom Tool-Executor schreiben

ğŸš€ **Das System kann sich jetzt selbst erweitern - mit Live-Internet-Zugriff!**
