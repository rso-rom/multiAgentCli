# ðŸš€ Quick Start - Mit Memory-Features

> **Schnelleinstieg in 5 Minuten | WÃ¤hle dein Backend**

## ðŸ“‹ Voraussetzungen

**WÃ¤hle EINES der folgenden Backends:**

| Backend | Empfohlen fÃ¼r | Setup-Zeit | Anleitung |
|---------|---------------|-----------|-----------|
| **LM Studio** | Desktop, GUI, Windows | 3 Min | **[â†’ LM Studio Setup](docs/setup/lm-studio.md)** |
| **Ollama Docker** | Server, CLI, Automation | 5 Min | **[â†’ Ollama Docker Setup](docs/setup/ollama-docker.md)** |

**Nicht sicher?** â†’ **[Backend-Vergleich & Empfehlung](docs/setup/backend-comparison.md)**

---

## ðŸŽ¯ Option A: LM Studio (Empfohlen fÃ¼r Einsteiger)

### Schritt 1: LM Studio starten (30 Sekunden)

1. Ã–ffne **LM Studio**
2. WÃ¤hle ein Modell (z.B. `mistral-7b-instruct-v0.3`)
3. Gehe zu **Local Server** Tab
4. Klicke **Start Server**
5. Warte bis "Server started on port 1234" erscheint

**Nicht installiert?** â†’ **[LM Studio Download & Installation](docs/setup/lm-studio.md#installation)**

### Schritt 2: .env konfigurieren
```env
# Bereits konfiguriert fÃ¼r LM Studio:
OLLAMA_URL=http://localhost:1234/v1
MODEL_BACKEND=ollama
```

### Schritt 3: Build & Start (30 Sekunden)

```bash
npm run build
npm start repl
```

**VollstÃ¤ndige Anleitung**: **[â†’ LM Studio Setup Guide](docs/setup/lm-studio.md)**

---

## ðŸŽ¯ Option B: Ollama Docker (FÃ¼r CLI-Nutzer)

### Schritt 1: Docker Container starten (2 Minuten)

```bash
# Container starten
docker-compose up -d ollama

# Warte bis Download fertig
docker ps  # PrÃ¼fe Status
```

### Schritt 2: Modell laden (3 Minuten)

```bash
# Empfohlenes Modell
docker exec -it codechat-ollama ollama pull mistral:7b

# Oder kleineres Modell fÃ¼r Tests
docker exec -it codechat-ollama ollama pull llama3.2:3b
```

### Schritt 3: .env konfigurieren
```env
# Bereits konfiguriert fÃ¼r Ollama:
OLLAMA_URL=http://localhost:11434
MODEL_BACKEND=ollama
OLLAMA_MODEL=mistral:7b
```

### Schritt 4: Build & Start

```bash
npm run build
npm start repl
```

**VollstÃ¤ndige Anleitung**: **[â†’ Ollama Docker Setup Guide](docs/setup/ollama-docker.md)**

---

## ðŸ§ª Teste Memory-Features (beide Optionen gleich)

**Test A: Prompt-History mit Semantic Search**
```
> ask Wie erstelle ich einen Webshop mit React?
â¤´ï¸ Asking model...
[Antwort kommt...]

> ask Was sind die Vorteile von TypeScript?
â¤´ï¸ Asking model...
[Antwort kommt...]

> ask ErklÃ¤re mir das MVC Pattern
â¤´ï¸ Asking model...
[Antwort kommt...]

> history webshop
ðŸ“œ Prompt History

1. [2025-10-31 19:35] ask (85% match)
   "Wie erstelle ich einen Webshop mit React?"

> history pattern
ðŸ“œ Prompt History

1. [2025-10-31 19:36] ask (92% match)
   "ErklÃ¤re mir das MVC Pattern"
```

**Test B: Memory-Workflow**
```
> orchestrate memory-test-workflow.yml

ðŸš€ Starting workflow: memory-test
ðŸ“ Testet alle 4 Memory-Ebenen

ðŸ¤– [knowledge-keeper] (Wissensspeicher)
   Task: Speichere Design Patterns...
[Speichert Singleton, Factory, Observer, Strategy]

ðŸ¤– [knowledge-retriever] (Wissensabrufer)
   Task: Welches Pattern fÃ¼r Logging-Klasse?
[Nutzt gespeichertes Wissen: Empfiehlt Singleton!]

ðŸ¤– [analyst] (Analyst)
   Task: Analysiere und empfehle...
[Analysiert: Singleton ist beste Wahl fÃ¼r Logging]

âœ… Workflow completed
```

---

## ðŸ§ª Was du jetzt testen kannst

### 1. Semantic Search in Action
```
> ask Wie funktioniert OAuth2 Authentication?
> ask Was ist JWT?
> ask ErklÃ¤re REST API Security

> history authentication
# Findet alle 3 Prompts, obwohl nur einer "authentication" enthÃ¤lt!
```

### 2. Memory Ã¼ber Sessions
```
Session 1:
> ask Merke dir: Ich bevorzuge React mit TypeScript
> exit

Session 2 (neu starten):
> ask Welches Framework bevorzuge ich?
# Agent erinnert sich! (Mid-term Memory)
```

### 3. Webshop-Projekt mit Memory
```
> orchestrate webshop-arc42-workflow.yml

# Workflow lÃ¤uft...
# Speichert Architektur, Code, Dokumentation in Memory

# SpÃ¤ter:
> history arc42
# Findet das Projekt!

> ask Zeige mir die Webshop-Architektur vom letzten Projekt
# Agent kann darauf zugreifen!
```

---

## ðŸ“Š Memory Dashboard

**Qdrant Web UI:**
```
http://localhost:6333/dashboard
```

Hier siehst du:
- ðŸ“¦ Collections (Deine Datenbanken)
- ðŸ”¢ Anzahl gespeicherter Vektoren
- ðŸ“ˆ Storage Statistiken

**API Check:**
```bash
curl http://localhost:6333/collections
```

Zeigt alle Memory-Collections.

---

## ðŸŽ® Interaktive Tests

### Test 1: Code-Speicherung
```bash
> load beispiel.ts
âœ… Loaded beispiel.ts

> ask Analysiere diesen Code und speichere Best Practices im Long-term Memory
â¤´ï¸ Asking model...
[Analysiert und speichert...]

# SpÃ¤ter in anderem Projekt:
> ask Was waren die TypeScript Best Practices?
[Findet gespeicherte Best Practices!]
```

### Test 2: Wissens-Akkumulation
```bash
# Frage 1:
> ask Was ist das Singleton Pattern?

# Frage 2:
> ask Was ist das Factory Pattern?

# Frage 3:
> ask Was ist das Observer Pattern?

# Jetzt suche:
> history pattern
# Findet alle 3!

# Oder:
> ask Vergleiche alle Design Patterns, die wir besprochen haben
# Agent nutzt Long-term Memory!
```

### Test 3: Multi-Agent mit Shared Memory
```bash
> orchestrate memory-test-workflow.yml

# Agents teilen Wissen Ã¼ber Memory:
# Agent 1 â†’ speichert
# Agent 2 â†’ liest aus Memory
# Agent 3 â†’ analysiert gesamtes Memory
```

---

## ðŸ” Monitoring

### Schaue was gespeichert wird:

**LMDB (Local Files):**
```bash
dir memory
```

**Qdrant (Vector DB):**
```bash
# Collections anzeigen
curl http://localhost:6333/collections

# Collection Details
curl http://localhost:6333/collections/ask-store
```

---

## âš¡ Performance-Tipps

### FÃ¼r schnellere Embeddings:
1. **Kleineres Embedding-Model** in LM Studio laden
2. **Batch Embeddings** nutzen (automatisch)
3. **Cache nutzen** (automatisch aktiviert)

### FÃ¼r bessere Semantic Search:
1. **Detailliertere Prompts** nutzen
2. **Kontext hinzufÃ¼gen**
3. **Tags verwenden** beim Speichern

---

## ðŸŽ¯ NÃ¤chste Schritte

### Grundlagen testen:
```bash
npm start repl
> ask Wie funktioniert OAuth2?
> history oauth
```

### Workflows testen:
```bash
> orchestrate memory-test-workflow.yml
> orchestrate webshop-arc42-workflow.yml
```

### Eigene Workflows erstellen:
Kopiere `memory-test-workflow.yml` und passe an!

---

## ðŸ“š VollstÃ¤ndige Dokumentation

### Backend-Setup
- **[LM Studio Setup Guide](docs/setup/lm-studio.md)** - VollstÃ¤ndige Anleitung mit Modellen, Settings, Troubleshooting
- **[Ollama Docker Setup Guide](docs/setup/ollama-docker.md)** - Container-Setup, CLI, Automatisierung
- **[Backend-Vergleich](docs/setup/backend-comparison.md)** - Welches Backend ist richtig fÃ¼r mich?

### Features
- **[Memory Testing Guide](docs/features/memory-system.md)** - 4-Ebenen Memory, Semantic Search, Qdrant
- **[OAuth2 Anleitung](docs/features/oauth.md)** - Login mit Google, GitHub, Token-Management

### Ãœbersicht
- **[README](README.md)** - Projekt-Ãœbersicht, Features, Architektur

---

## ðŸ†˜ Hilfe

### Qdrant startet nicht:
```bash
docker-compose restart qdrant
docker logs codechat-qdrant
```

### LM Studio verbindet nicht:
- Server lÃ¤uft auf Port 1234?
- Model geladen?
- In .env: `OLLAMA_URL=http://localhost:1234/v1`

### Memory speichert nicht:
- Qdrant lÃ¤uft? `docker ps | findstr qdrant`
- In .env: `USE_QDRANT=true`
- In .env: `ASK_STORE_ENABLED=true`

---

## âœ… Zusammenfassung

**Du hast jetzt:**
- âœ… Qdrant lÃ¤uft (Docker)
- âœ… 4 Memory-Ebenen aktiviert
- âœ… Semantic Search funktioniert
- âœ… Prompt-History mit Suche
- âœ… Test-Workflows bereit

**Starte jetzt:**
```bash
# 1. LM Studio Server starten (Port 1234)
# 2. Dann:
npm start repl
> orchestrate memory-test-workflow.yml
```

**Viel Erfolg! ðŸš€**
